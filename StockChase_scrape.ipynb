{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:55:32.962804Z",
     "start_time": "2019-10-04T12:55:32.940000Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T12:55:33.551110Z",
     "start_time": "2019-10-04T12:55:33.538031Z"
    }
   },
   "outputs": [],
   "source": [
    "URL = \"https://stockchase.com/opinions/recenttop/sort/date/page/1/direction/desc/max/15\"\n",
    "PARSER = 'html.parser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T13:07:30.169210Z",
     "start_time": "2019-10-01T13:07:30.127175Z"
    }
   },
   "outputs": [],
   "source": [
    "def soup_maker(url, parser):\n",
    "    \"\"\"This function creates a soup from a url\"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, parser)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T17:13:23.546954Z",
     "start_time": "2019-10-04T17:13:23.286126Z"
    }
   },
   "outputs": [],
   "source": [
    "TEMP_TRADE_TSX = ['-T', 'T ']\n",
    "TEMP_TRADE_US = ['-Q','-N', '-O', '-A' ]\n",
    "\n",
    "def modify_ticker(my_ticker, TRADE_US = ['-Q','-N', '-O', '-A' ]):\n",
    "    if my_ticker[-2:] in TRADE_US:\n",
    "        return my_ticker[0:-2]\n",
    "    elif my_ticker[-2:] == 'US':\n",
    "        return my_ticker[0:-3]\n",
    "    elif my_ticker[-2:] == '-T':\n",
    "        return 'TSE:' + my_ticker[0:-2]\n",
    "    elif my_ticker[-2:] == 'T ':\n",
    "        return 'TSE:' + my_ticker[0:-3]\n",
    "    else:\n",
    "        return 'CASH'\n",
    "\n",
    "class StockChasePageInfo(object):\n",
    "    def __init__(self, url, parser):\n",
    "        \"\"\"This is the constructor for the triangle class.\"\"\"\n",
    "        self.page = requests.get(url)\n",
    "        self.soup = BeautifulSoup(self.page.content, parser)\n",
    "        \n",
    "    \n",
    "    def get_expert_names(self):\n",
    "        \"\"\"This method returns the list of all experts on the page\"\"\"\n",
    "        findall_experts = self.soup.find_all('a', class_=\"expert-name\")\n",
    "        experts_series = pd.Series(findall_experts)\n",
    "\n",
    "        experts_list_text = experts_series.apply(lambda x:x.text.strip())\n",
    "\n",
    "        return experts_list_text\n",
    "    \n",
    "    \n",
    "    def get_expert_titles(self):\n",
    "        \"\"\"This method returns the list of all expert titles on the page\"\"\"\n",
    "        findall_expert_titles= self.soup.find_all('div', class_=\"expert-title\")\n",
    "        expert_titles_series = pd.Series(findall_expert_titles)\n",
    "\n",
    "        expert_titles_list_text = expert_titles_series.apply(lambda x:x.text.strip())\n",
    "\n",
    "        return expert_titles_list_text\n",
    "    \n",
    "    \n",
    "    def get_expert_tickers(self):\n",
    "        \"\"\"This method returns the list of all expert tickers on the page\"\"\"\n",
    "        findall_expert_tickers= self.soup.find_all('span', class_=\"opinion-mini__symbol\")\n",
    "        expert_tickers_series = pd.Series(findall_expert_tickers)\n",
    "\n",
    "        expert_tickers_list_text = expert_tickers_series.apply(lambda x:x.text.strip(\"()\"))\n",
    "        \n",
    "        expert_tickers_list_ready = expert_tickers_list_text.apply( lambda x: modify_ticker(x))\n",
    "        \n",
    "        expert_tickers_list_ready = expert_tickers_list_ready.apply( lambda x: x.replace('.', '-'))\n",
    "\n",
    "        return expert_tickers_list_ready\n",
    "    \n",
    "    \n",
    "    def get_expert_picks(self):\n",
    "        \"\"\"This method returns the list of all expert picks on the page\"\"\"\n",
    "        findall_expert_picks= self.soup.find_all('span', class_=\"opinion-mini__name\")\n",
    "        expert_picks_series = pd.Series(findall_expert_picks)\n",
    "\n",
    "        expert_picks_list_text = expert_picks_series.apply(lambda x:x.text.strip())\n",
    "\n",
    "        return expert_picks_list_text\n",
    "    \n",
    "    \n",
    "    def get_expert_dates(self):\n",
    "        \"\"\"This method returns the list of all expert dates on the page\"\"\"\n",
    "        findall_expert_dates= self.soup.find_all('div', class_=\"opinion-mini__date\")\n",
    "        expert_dates_series = pd.Series(findall_expert_dates)\n",
    "\n",
    "        expert_dates_list_text = expert_dates_series.apply(lambda x:x.text.strip())\n",
    "        \n",
    "        expert_dates_list_dt = expert_dates_list_text.apply(lambda x: datetime.strptime(x, '%B %d, %Y').date())\n",
    "\n",
    "        return expert_dates_list_dt\n",
    "    \n",
    "    \n",
    "    def get_expert_comments(self):\n",
    "        \"\"\"This method returns the list of all expert comments on the page\"\"\"\n",
    "        findall_expert_comments= self.soup.find_all('div', class_=\"opinion-mini__comment\")\n",
    "        expert_comments_series = pd.Series(findall_expert_comments)\n",
    "\n",
    "        expert_comments_list_text = expert_comments_series.apply(lambda x:x.text.strip())\n",
    "\n",
    "        return expert_comments_list_text\n",
    "    \n",
    "    \n",
    "    def get_prices(self):\n",
    "        \"\"\"This method returns the list of all expert prices on the page\"\"\"\n",
    "        findall_prices= self.soup.find_all('div', class_=\"expert-cell d-none d-lg-block\")\n",
    "        prices_series = pd.Series(findall_prices)\n",
    "\n",
    "        prices_list_text = prices_series.apply(lambda x:x.find_all('div')[3].text.strip())\n",
    "\n",
    "        return prices_list_text\n",
    "      \n",
    "    \n",
    "    def get_all(self):\n",
    "        \"\"\"This method returns all the data on the page\"\"\"\n",
    "        df = pd.DataFrame()\n",
    "        df['Expert Name'] = self.get_expert_names()\n",
    "        df['Expert Title'] = self.get_expert_titles()\n",
    "        df['Top Pick'] = self.get_expert_picks()\n",
    "        df['Ticker'] = self.get_expert_tickers()\n",
    "        df['Price'] = self.get_prices()\n",
    "        df['Date'] = self.get_expert_dates()\n",
    "        df['Comments'] = self.get_expert_comments()\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T16:45:50.233683Z",
     "start_time": "2019-10-13T16:45:50.188092Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "LAST_PAGE = 1798\n",
    "\n",
    "\n",
    "def scrape_all_stock_chase_top_picks(mod_ticker_func, LAST_PAGE,\\\n",
    "                            URL = \"https://stockchase.com/opinions/recenttop/sort/date/page/1/direction/desc/max/15\",\\\n",
    "                            PARSER = 'html.parser', save=True, filename = 'stockchase_all_top_picks'):\n",
    "    info_dict = {} \n",
    "    modify_ticker = mod_ticker_func\n",
    "    for i in tqdm(range(LAST_PAGE)):\n",
    "        print(\"Loading Page {}...\".format(i+1))\n",
    "        p_URL = URL.replace('/1/','/'+str(i+1) + '/')\n",
    "        p=StockChasePageInfo(p_URL, PARSER)\n",
    "        info_dict['p'+ str(i+1)] = p.get_all()\n",
    "    if save:\n",
    "        print(\"Saving to CSV...\")\n",
    "        my_df=pd.concat(info_dict.values())\n",
    "        my_df.to_csv(filename + '.csv')\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
